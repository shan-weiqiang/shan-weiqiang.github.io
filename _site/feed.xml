<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-18T18:30:01+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">shanweiqiang’s blog</title><subtitle></subtitle><entry><title type="html">CMake: Best Practice(bp)</title><link href="http://localhost:4000/2024/09/18/cmake-best-practice.html" rel="alternate" type="text/html" title="CMake: Best Practice(bp)" /><published>2024-09-18T09:22:46+08:00</published><updated>2024-09-18T09:22:46+08:00</updated><id>http://localhost:4000/2024/09/18/cmake-best-practice</id><content type="html" xml:base="http://localhost:4000/2024/09/18/cmake-best-practice.html"><![CDATA[<p>It has been a long time since I want to summarize the usage of CMake and give a best practice for using it. Recently I have time to read the book: <a href="https://crascit.com/">Professional CMake: A Practical Guide</a>, and it’s time to do this. CMake is complex and easy at the same time: it’s complex because what it tries to solve is complex; it’s easy because once we know how to use it and familiar with the best practice, it’s basically repitition afterwards. So the key here is to have a model for repetition, which I try to give here.</p>

<p>All code can be found at repo: <a href="https://github.com/shan-weiqiang/cmake/tree/main/best_practice_lib">best_practice_lib</a>.</p>

<p>For easy understanding of the repo, the project composition is like following:</p>

<p><img src="/assets/images/cmake_best_practice.png" alt="alt text" /></p>

<p>After build and installation, the package looks like following:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  <span class="nb">install </span>git:<span class="o">(</span>main<span class="o">)</span> ✗ tree                                  //&lt;<span class="nb">install </span>folder&gt;
<span class="nb">.</span>
├── include                                                   // header files folder
│   ├── caculator                                             
│   │   └── caculator.h
│   ├── divide
│   │   └── divide.h
│   ├── json.hpp
│   └── multi
│       └── multi.h
└── lib
    ├── cmake                                                 // folder <span class="k">for </span>cmake scripts
    │   └── bp
    │       ├── bpConfig.cmake                                // package level cmake
    │       ├── bpConfigVersion.cmake                         // package level version cmake<span class="o">(</span>package only<span class="o">)</span>
    │       ├── Caculator                                     // Caculator component
    │       │   ├── CaculatorConfig.cmake
    │       │   └── CaculatorConfig-noconfig.cmake
    │       ├── Json                                          // Json component
    │       │   └── JsonConfig.cmake
    │       └── Math                                          // Math component
    │           ├── MathConfig.cmake
    │           └── MathConfig-noconfig.cmake
    ├── libCaculator.so -&gt; libCaculator.so.3                  // libraries <span class="k">in </span>unix version format  
    ├── libCaculator.so.3 -&gt; libCaculator.so.3.2.1
    ├── libCaculator.so.3.2.1
    ├── libdivide.so -&gt; libdivide.so.3
    ├── libdivide.so.3 -&gt; libdivide.so.3.2.1
    ├── libdivide.so.3.2.1
    ├── libmulti.so -&gt; libmulti.so.3
    ├── libmulti.so.3 -&gt; libmulti.so.3.2.1
    └── libmulti.so.3.2.1
</code></pre></div></div>

<p>Consumers can use above package and components and specify versions like:</p>

<div class="language-cmake highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">find_package</span><span class="p">(</span>bp 2.1.0 EXACT COMPONENTS Caculator<span class="p">)</span>
</code></pre></div></div>
<p>Examples can be found at: <a href="https://github.com/shan-weiqiang/cmake/tree/main/best_practice_client">best_practice_client</a></p>

<p>What really matters:</p>

<ul>
  <li>All target dependencies are managed automatically. For example when client use <code class="language-plaintext highlighter-rouge">Math::Caculator</code> component, it’s dependency <code class="language-plaintext highlighter-rouge">Math::divide</code>, <code class="language-plaintext highlighter-rouge">Math::multi</code>, and their dependencies <code class="language-plaintext highlighter-rouge">nlomann::json</code> and <code class="language-plaintext highlighter-rouge">foonathan_memory</code> will be automatically discovered. If the dependencies are not met, error will happen during configuration time and give proper messsages.</li>
  <li>Support <code class="language-plaintext highlighter-rouge">QUIET</code> option for package</li>
</ul>

<p>Detailed usage and their nationale are given inside the repo as comments. The repo will be continously updated in the future, such as <code class="language-plaintext highlighter-rouge">ctest</code>, <code class="language-plaintext highlighter-rouge">cpack</code> will be added. Hope it can save some time for you.</p>]]></content><author><name></name></author><category term="c++" /><summary type="html"><![CDATA[It has been a long time since I want to summarize the usage of CMake and give a best practice for using it. Recently I have time to read the book: Professional CMake: A Practical Guide, and it’s time to do this. CMake is complex and easy at the same time: it’s complex because what it tries to solve is complex; it’s easy because once we know how to use it and familiar with the best practice, it’s basically repitition afterwards. So the key here is to have a model for repetition, which I try to give here.]]></summary></entry><entry><title type="html">c++ container cheet sheets</title><link href="http://localhost:4000/2024/08/31/c++-container-cheet-sheets.html" rel="alternate" type="text/html" title="c++ container cheet sheets" /><published>2024-08-31T09:22:46+08:00</published><updated>2024-08-31T09:22:46+08:00</updated><id>http://localhost:4000/2024/08/31/c++-container-cheet-sheets</id><content type="html" xml:base="http://localhost:4000/2024/08/31/c++-container-cheet-sheets.html"><![CDATA[<p><img src="/assets/images/cplusplus_containers.png" alt="alt text" /></p>

<hr />
<p><strong>NOTE</strong></p>

<ul>
  <li><em>CopyConstructible</em> must be <em>MoveConstructible</em></li>
  <li><em>MoveConstructible</em> might not be <em>CopyConstructible</em></li>
  <li><em>CopyConstructible</em> have more strict requirement than <em>MoveConstructible</em></li>
  <li><a href="https://en.cppreference.com/w/cpp/named_req/Container">Reference</a></li>
  <li><a href="https://stackoverflow.com/questions/1436020/whats-the-difference-between-deque-and-list-stl-containers#:~:text=std%3A%3Alist%20is%20basically,performance%20characteristics%20than%20a%20list.">What’s the difference between deque and list STL containers?</a></li>
</ul>

<hr />]]></content><author><name></name></author><category term="c++" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">type system and language bindings</title><link href="http://localhost:4000/2024/07/14/understanding-types.html" rel="alternate" type="text/html" title="type system and language bindings" /><published>2024-07-14T10:20:46+08:00</published><updated>2024-07-14T10:20:46+08:00</updated><id>http://localhost:4000/2024/07/14/understanding-types</id><content type="html" xml:base="http://localhost:4000/2024/07/14/understanding-types.html"><![CDATA[<p>Everyday, I deal with all kinds of types: C++ types, Python types, JSON, XML, Protocol Buffers, IDL, ROS msg… They are all representation of types. Some of them are static, others are dynamic. If you think about them carefully, there are a lot going on beneath the surface. There are some facts that are rather counter intuitive.</p>

<ul id="markdown-toc">
  <li><a href="#type-systems" id="markdown-toc-type-systems">Type Systems</a>    <ul>
      <li><a href="#static" id="markdown-toc-static">Static</a></li>
      <li><a href="#introspection" id="markdown-toc-introspection">Introspection</a></li>
      <li><a href="#reflection" id="markdown-toc-reflection">Reflection</a></li>
      <li><a href="#dynamic-type" id="markdown-toc-dynamic-type">Dynamic type</a></li>
      <li><a href="#dynamic-data" id="markdown-toc-dynamic-data">Dynamic data</a></li>
    </ul>
  </li>
  <li><a href="#schema-languagedata-definition-languages" id="markdown-toc-schema-languagedata-definition-languages">Schema Language/Data Definition Languages</a></li>
  <li><a href="#language-binding" id="markdown-toc-language-binding">Language binding</a></li>
  <li><a href="#self-hosting" id="markdown-toc-self-hosting">Self-hosting</a></li>
  <li><a href="#schema-and-data" id="markdown-toc-schema-and-data">Schema and Data</a></li>
  <li><a href="#interpreterdynamic-types-systems" id="markdown-toc-interpreterdynamic-types-systems">Interpreter/Dynamic types systems</a>    <ul>
      <li><a href="#omg-xtypes" id="markdown-toc-omg-xtypes">OMG XTypes</a></li>
      <li><a href="#protocol-buffers-dynamic-type" id="markdown-toc-protocol-buffers-dynamic-type">Protocol Buffers dynamic type</a></li>
    </ul>
  </li>
</ul>

<h2 id="type-systems">Type Systems</h2>

<p>Syntax rules for defining data types. It consists of pre-defined basic types and rules to build complex customized data types. It is compiled into machine code for static-typed languages like C/C++, or dynamically loaded as types in dynamic-typed languages like Python.</p>

<p>It includes language-specific type system and language-neutral type systems that are defined by interface definition languages, which we will have detailed discussion later.</p>

<p>I categorize type system in following way, according to their runtime behavior: static type system, introspection type system, dynamic type system, dynamic data system.</p>

<h3 id="static">Static</h3>

<p>The main characteristic of those type system is that after compiling, all type information is lost. Type information of a variable, like type name, type, member name, member type, are translated by compiler into machine code directly. Those types only lives <em>before</em> compilation. Representitives of this kind of types is C.</p>

<h3 id="introspection">Introspection</h3>

<p><em>Introspection</em> means that type information can be retrieved at <em>runtime</em>. This means that type information of a variable lives at runtime. We can get the type information through specific API. To achieve this, static variables and functions are required for a specific type. Those <em>introspection</em> variables and codes are compiled into text and data segment of ELF file. At runtime, caller need to know the type name(string literal, for example) to get the relevent introspection information. An example is ROS2 type system. In ROS2, the ROS2 compiler will compile the <em>.msg</em> file into language-specific type representations. At the same time, introspection codes and static variables that store type information for every type are generated. Every type is identified by it’s unique <em>path</em>(string literal) and at runtime, by using this <em>path</em>, the introspection information can be retrieved(this normally involves global function naming convention together with the type path, and the use of <em>dlopen</em> and <em>dlsym</em> to find symbols in shared libraries, which is how ROS2 support introspection). C++ is mostly static types, however it can use RTTI to support <em>introspection</em>.</p>

<h3 id="reflection">Reflection</h3>

<p><em>The ability to inspect the code in the system and see object types is not reflection, but rather Type Introspection. Reflection is then the ability to make modifications at runtime by making use of introspection. The distinction is necessary here as some languages support introspection, but do not support reflection. One such example is C++.</em> <a href="https://stackoverflow.com/questions/37628/what-is-reflection-and-why-is-it-useful">source</a>. According to this definition, reflection supports modification of the <em>values</em> and <em>types</em> through <em>introspection</em>.</p>

<h3 id="dynamic-type">Dynamic type</h3>

<p>Introspection and Reflection can be implemented <em>statically</em> or <em>dynamically</em>. Like we mentioned above, ROS2 supports <em>introspection</em> and <em>reflection</em> statically, since all the codes and static variables that contains type information are <em>statically</em> generated and compiled into machine code at compile time.</p>

<p>What if we can read those type information at <em>runtime</em>, without knowing the type information at compile time? This is dynamic type system. It consists of <em>statically</em> compiled data structures and codes to represent all possible types at runtime, and <em>type representation</em> format (.msg, .proto, .idl, xml, json, etc) to store type information and to be read by the before-mentioned static program. The static program is called <em>dynamic type</em> system and it will dynamically build types based on any kinds of <em>type representation</em>, as long as it contains valid type information. We will talk about dynamic types in more detail later, take the XTypes of OMG as example.</p>

<h3 id="dynamic-data">Dynamic data</h3>

<p>It’s not enough to only have dynamic types. To dynamically represent <em>values</em> at runtime for a dynamic type, <em>dynamic data</em> system is required. Dynamic data system use dynamic type system and carries <em>values</em> for each type. Note that statically compiled data and dynamically created data of the same type often has different memory model. To illustrate this, let’s consider one .idl struct <em>StructExample</em>. The <em>StructExample</em> should be compiled into C++ code by tools like <em>fastddsgen</em> and a C++ class is generated for this specific struct and have determined memory model at compile time. The same .idl <em>StructExample</em> can also be loaded at runtime by XTypes dynamic type system, without knowing the type at compile time. In both way, we can create the same value for <em>StructExample</em> type, but with very different memory layout. The statically generated C++ class will have memory layout according to the language specific rules. While the memory layout of the dynamic data will have memory layout according to the implementation of the dynamic type/data system. We will illustrate this points again in later chapter.</p>

<h2 id="schema-languagedata-definition-languages">Schema Language/Data Definition Languages</h2>

<p>Language/platform neutral way for defining data types.  It consists of pre-defined basic types and rules to build complex customized data types. It is mainly used for data exchange and storage across different languages and platforms. It is compiled into different language code representations, or dynamically loaded as dynamic types.  The most familiar schema languages in the world might be XML Schema(XSD) and JSON Schema. The .xml and .json files we encounter every day are NOT schemas, they are <em>data</em> of a XML Schema and JSON Schema respectively. We normally do not deal with the XML Schema or JSON Schema when we write a .xml or .json file, instead we directly write <em>data</em> of those schemas. This is because <em>data</em> or <em>serialized data</em> of  XML Schema and JSON Schema self-contains the XML Schema or JSON Schema. All kinds of XML or JSON parsers can get all the schema information from the <em>data</em> to dynamically build types to interpret the <em>data</em>. Compared with Protocol Buffers, a .json file is NOT the counterpart of .proto file, instead it is the counter-part of on-wire data of Protocol Buffers. It’s rather counter intuitive. The reason behind this is that both XML and JSON only support and serialize it’ data into human readable text format and it’s data contains all the info of it’s schema, while Protocol Buffers serialize it’s data into binary format. Of course it’s ok if Protocol Buffers choose to use plain text like JSON to encode it’s on-wire format, but it’s in the expense of speed and efficiency. In exchange for speed and efficiency, it’s is required in the receiver side to know the schema to decode the on-wire data, unless the schema itself is encoded and send together with data. We will talk about this latter.</p>

<h2 id="language-binding">Language binding</h2>

<p>For schema languages to run on machine, it has to be bound to specific programming languages. For a dynamic-typed languages like Python, loading built-in type system and loading schema languages are very similar, only needing to transform schema language representation into type system of the corresponding language types. To the interpreter, they are all string tokens. For the static-types languages, the schema language must be firstly compiled into type representation of the specific language, then compiled into machine code. Static-typed languages can also implement programs to dynamically load schema languages. This program can be seen as counterpart of interpreter of dynamic-typed languages.</p>

<h2 id="self-hosting">Self-hosting</h2>

<p>Type system or schema language is self-hosting if it can use  it’s type or schema to describe  other types or schemas. It’s a rather tricky concept. The type system or schema language should not be too simple as to not have enough expressiveness to describe all it’s features. At the same time, it should not be too complex as to make it impossible for it to describe itself. It’s like a competition between expressiveness and complexcity of the type system and the schema language itself. For example, C++ type system is so complex that we normally do not use C++ types to represent another C++ type, even given that C++ type’s great expressiveness. Protocol Buffers is not that complex and have enough expressiveness so that we can use one single <em>descriptor.proto</em> schema to describe all possible other Protocol Buffers schemas, making Protocol Buffers self-hosting.</p>

<h2 id="schema-and-data">Schema and Data</h2>

<p>After we understand what is self-hosting, we better take the chance to have a deep look at what is <em>schema</em>  and what is <em>data</em>. Simply put: <em>data</em> is actualization of <em>schema,</em> and for self-hosting schema languages, we can use <em>data</em>  of one special schema to represent another schema, we call this special schema, <em>meta-schema</em>, or, <em>schema of schemas</em>. For schema languages like XML Schema, or JSON Schema, there is no need for the meta-schema, since every data can contains it’s schema in itself. We do not need another special schema to carry the schema information of it’s data. But for Protocol Buffers and OMG XTypes, the schema info is not carried in every encoded data. One way to carry schema info is to encode all schema info into data, which like said before is inefficient. Another way is to use the text format .proto or .idl file directly, which is feasible but also inefficient. The final way that adopted both by Protocol Buffers and OMG XTypes is to use it’s self-hosting feature to define a meta-schema that can carry information of another schema. This way, the schema information can be encoded <em>the same</em> with data and can be transmitted on-wire. This meta-schema is often a built-in schema in those schema languages, for example <em>type_object.idl</em>, or <em>descriptor.proto</em>, and their compiled C++ class are <em>TypeObject</em> and <em>DescriptorProto,</em> in XTypes and Protocol Buffers respectively. Instance of those class carries the same information as a proto or idl file. In dynamic type systems we will latter talk about, those built-in schemas will be used as input to build dynamic types, since they are equal to schema files. In Protocol Buffers, the <em>DescriptorProto</em> calss will be based to construct <em>Descriptor</em> class, which represents a dynamic type. In XTypes, <em>TypeObject</em> will be based to construct a <em>DynamicType</em>. Based on those dynamic types, <em>dynamic data</em> can be realized and be used to decode schema data dynamically.</p>

<h2 id="interpreterdynamic-types-systems">Interpreter/Dynamic types systems</h2>

<p>In our context this seems not the same as dynamic type in dynamic-typed language. Here we specifically mean <em>dynamically representing the value of a type.</em> While in dynamic-typed language context, dynamic type means a type of a variable can change dynamically. However, deep down, they are the same thing, both operating types at runtime, while the operator is statically compiled program, normally called interpreter. One of the example of the implementation of dynamic type system is the JSON Schema parser called <a href="https://github.com/nlohmann/json">nlohmann json</a>, it can dynamically read json file and parse it into C++ native types. Other dynamic type systems such as OMG XTypes, Protocol Buffers Reflection have similar functionality.</p>

<h3 id="omg-xtypes">OMG XTypes</h3>

<p>OMG XTypes is a dynamic data type system used by DDS. <a href="https://www.omg.org/spec/DDS-XTypes/1.3/PDF">The standard</a> is very clear about it’s internal structure. The simplified version is that it contains  following concepts:</p>

<ul>
  <li>Type System: This is the most abstract and the most important part of XTypes. It defines the basic types and the structure of how to construct complex data types. Also defines how to manage modules. This definition composes the core of the type system. From this type system comes everything else.</li>
  <li>Type Representation: Once we have a type system, we need to find a way to represent types properly. XTypes can be represented using IDL, XML, XSD, <em>TypeObject</em> and <em>TypeIdentifier</em>, they both can contain the same information. Note that <em>TypeObject</em> and <em>TypeIdentifier</em> represent types at runtime. Those are the <em>schema</em>s.</li>
  <li>Data Representation: XTypes support CDR encoding to represent data of schema</li>
</ul>

<p>Until now, there is no dynamic types involved. Like we said earlier, it’s not enough to only have type system and schema languages. We now need to bind it into specific programming languages. XTypes provide two kinds of language bindings, and one of them is dynamic language binding.</p>

<ul>
  <li>Plain language binding: For this binding, schemas will be compiled into specific language code by the XTypes compiler. For example, <em>fastddsgen</em>. The generated code is then compiled into user application. This should be the normal way of using XTypes for the end user, since it is the most fast and efficient way. Note that for dynamic languages, the compile code might be just a different version of the original schema, since dynamic languages by nature interpret types dynamically.</li>
  <li>Dynamic language binding: For this binding, schemas need not to be compiled into specific language code like C++, instead, a dynamic type program need to be developed to interpret the schemas dynamically. This dynamic type program can be compared with the interpreter of the dynamic languages.</li>
</ul>

<p>Note that the memory model for representing the data type in these two types of bindings are very different. For the plain language binding, the data might be contagious since it is represented using specificaly generated language types, like C++, the type is represented by a individual class. The memory model is decided by the generated class. For dynamic language binding, since there are no individual language specific data type to represent the data type, the memory model is decided by the implementation of the dynamic type program.</p>

<p>The dynamic type system mainly contain four parts. The first one is dynamic type system, this system should build starting from basic types and should recursively contains itself to support complex user defined types. Dynamic types system is built hierarchically, with each level a specific type kind and if one level contains members, it should contain another dynamic type recursively. This way the dynamic type itself is static, but at runtime it can represent any types that are defined in a schema. The second one is a dynamic data system. If we only have dynamic type system, it is not enough to decode data and inspect them in human readable way. Dynamic data system must use dynamic type and also recursively include itself to represent complex value of dynamic types. An instance of dynamic type is a specific type. An instance of dynamic data is a specific data of a specific type. Dynamic data is also static code. As you can imagine, there are lots of recursions going on here. The third one is the above mentioned <em>TypeObject</em>, which contain schema information. XTypes use <em>TypeObject</em> as source to build dynamic types. The final part is a global type management system. Types are inter-dependent. One types can depend on another and this is generally the normal way of how types are constructed. A global instance that manages all types, that recieves type registration, that create dynamic types is necessary to coordinate all dynamic type management.</p>

<h3 id="protocol-buffers-dynamic-type">Protocol Buffers dynamic type</h3>

<p>For Protocol Buffers, conceptually it’s the same with XTypes, with some differences.</p>

<ul>
  <li>Type system: Protocol Buffers have it’s own type system. Basic types, ways to construct complex user defined types and module management etc.</li>
  <li>Type representation: it’s more limited compared with XTypes, only <em>proto</em> files are supported.</li>
  <li>Data representation: Protocol Buffers use it’s own encoding format to encode data.</li>
</ul>

<p>As for language bindings, Protocol Buffers supports plain language binding and limited dynamic language binding.</p>

<ul>
  <li>Plain language binding: Protocol Buffers provide compiler to compile schema into language specific code representation, just like XTypes</li>
  <li>Dynamic language binding: Compared with XTypes, Protocol Buffers’ support for dynamic types is different and less complete. Protocol Buffers do have full representation at runtime using <em>Descriptor</em> class, but the dynamic type system is not hierarchically and recursively built like XTypes. Protocol Buffers use it’s reflection system to support dynamic types. When building a dynamic type, Protocol Buffers does not hierarchically create dynamic data like XTypes, instead, it first allocate one chunk of memory, then recursively resolve <em>Descriptor</em> and use inplacement new operator to assign the position of every data member and create cooresponding <em>Field</em>, finally it use the reflection system to build a <em>Message</em>, which is <em>the same</em> type with the plain language binding. As we can see, the memory layout for plain language binding and dynamical language binding in Protocol Buffers is very similar. The most apparent difference compared with XTypes is that the dynamically created message and the plain language binding message is of the same type!</li>
</ul>]]></content><author><name></name></author><category term="design_philosophy" /><summary type="html"><![CDATA[Everyday, I deal with all kinds of types: C++ types, Python types, JSON, XML, Protocol Buffers, IDL, ROS msg… They are all representation of types. Some of them are static, others are dynamic. If you think about them carefully, there are a lot going on beneath the surface. There are some facts that are rather counter intuitive.]]></summary></entry><entry><title type="html">arguments of clone() system call</title><link href="http://localhost:4000/2024/06/30/the-clone-sys-call.html" rel="alternate" type="text/html" title="arguments of clone() system call" /><published>2024-06-30T10:20:46+08:00</published><updated>2024-06-30T10:20:46+08:00</updated><id>http://localhost:4000/2024/06/30/the-clone-sys-call</id><content type="html" xml:base="http://localhost:4000/2024/06/30/the-clone-sys-call.html"><![CDATA[<p>This blog reference most of it’s content to Chapter 28.2 of <a href="https://man7.org/tlpi/">The Linux Programming Interface</a></p>

<p><code class="language-plaintext highlighter-rouge">clone()</code> system call has following signature:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">clone</span><span class="p">(</span><span class="kt">int</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">),</span> <span class="kt">void</span> <span class="o">*</span><span class="n">child_stack</span><span class="p">,</span> <span class="kt">int</span> <span class="n">flags</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">func_arg</span><span class="p">,</span> <span class="p">...</span> <span class="cm">/* pid_t *ptid, struct user_desc *tls, pid_t *ctid */</span> <span class="p">);</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">func</code> is the address of the entry function; <code class="language-plaintext highlighter-rouge">child_stack</code> is the starting pointer on which the new function’s stack will be built; <code class="language-plaintext highlighter-rouge">flags</code> are sets of bit masks that are used to specify the behaviors of this <code class="language-plaintext highlighter-rouge">clone</code> operation. We focus on some of the flags. Before starting to delve into the discussion of specific flags, it’s useful to have a general understanding of what is <code class="language-plaintext highlighter-rouge">process</code> and <code class="language-plaintext highlighter-rouge">thread</code> in linux. As far as i read, following quote from the book <a href="https://man7.org/tlpi/">The Linux Programming Interface</a> have the most concise and accurate description:</p>

<blockquote>
  <p>At this point, it is worth remarking that, to some extent, we are playing with
words when trying to draw a distinction between the terms thread and process. It
helps a little to introduce the term <em>kernel scheduling entity</em> (KSE), which is used in
some texts to refer to the objects that are dealt with by the kernel scheduler. Really,
threads and processes are simply KSEs that provide for greater and lesser degrees
of sharing of attributes (virtual memory, open file descriptors, signal dispositions,
process ID, and so on) with other KSEs. The POSIX threads specification provides
just one out of various possible definitions of which attributes should be shared
between threads.</p>
</blockquote>

<p>I could not describe the difference and relationship between <code class="language-plaintext highlighter-rouge">process</code> and <code class="language-plaintext highlighter-rouge">thread</code> better than this quote, so no more words about <code class="language-plaintext highlighter-rouge">process</code> and <code class="language-plaintext highlighter-rouge">thread</code>. Let’s go into some of the flags. Different flags combinations in the <code class="language-plaintext highlighter-rouge">clone()</code> call will create different KSEs that will share resources with the calling KSE in different level and aspects. In following discussion we avoid using <code class="language-plaintext highlighter-rouge">process</code> and <code class="language-plaintext highlighter-rouge">thread</code> to prevent ambiguity, instead KSE is used to denote the returned entity by <code class="language-plaintext highlighter-rouge">clone()</code></p>

<h1 id="clone_files">CLONE_FILES</h1>

<p>If specified the returned KSE shares the same table for descriptors, which means that file descriptor creation and deallocation is visible between each other. For example, if in the calling KSE there is a new <code class="language-plaintext highlighter-rouge">socket</code> created, it will be automatically usable in the returned KSE. This flag makes the calling KSE and the returned KSE not only share the <em>file description</em>, but also the <em>file descriptor</em>. Please note the difference: file descriptions can be referenced by <em>multiple</em> file descriptors, both in same process or in different process. If this flag is not specified, the returned KSE will have a <em>copy</em> of the calling KSE’s file descriptor table, which will increment the reference count for the file description that the file descriptors point to. In this scenario, two different file descriptors point to the same file description(system wide resource), and they share the properties that are decided by the file description, like read/write positions, but they are different file descriptors. If inside one KSE, the file descriptor is closed, the file descriptor in another KSE is still usable. But if CLONE_FILES is specified, the calling KSE and the returned KSE share the same file descriptor, not copy.</p>

<h1 id="clone_fs">CLONE_FS</h1>

<p>If specified, calling KSE and returned KSE share current working directory and root directory. If any one of them changes those value, the other one sees them. Again, if not specified, the returned KSE have a copy for that of the calling KSE and after the copy, they will have individual working directory and root directory, with change of them not affecting each other.</p>

<h1 id="clone_vm">CLONE_VM</h1>

<p>If specified, the calling KSE and the returned KSE share the same virtual memory table. Otherwise, the returned KSE get a copy of the calling KSE’s virtual table, like in <code class="language-plaintext highlighter-rouge">fork()</code></p>

<h1 id="clone_sighand">CLONE_SIGHAND</h1>

<p>If specified, the calling KSE and the returned KSE share the same handling behavior for every signal. If not specified, the returned KSE get a copy of current behavior from the calling KSE, but when any of them changes the signal behavior, the other one can not see it.</p>

<p>Pending signals and signal masks are NOT shared between the calling KSE and the returned KSE, even if this flag is specified. panding signals and signal masks are KSE specific.</p>

<p>Imagine that this flag is specified and both KSE share the same signal handler, when one of them changes the handler, so the handler address is changed, what happens if the other KSE get the signal and need to call this handler(which is changed by another KSE to a different address)?  The only way this works is that the two KSEs must have same virtual memory address. Say if one of the KSE load some library into the virtual memory and changes the handler address to this memory region, if the two KSEs share the same virtual memory, the other one can safely calls the handler, otherwise segmentation fault is supposed to happen. So if CLONE_SIGHAND is specified, CLONE_VM must also be specified.</p>

<h1 id="clone_thread">CLONE_THREAD</h1>

<p>If specifed the returned KSE have the same thread group ID as the calling KSE, otherwise a new thread group ID is created for the returned KSE. Thread group ID is the same thing as process ID. Following diagram illustrate the relationship between different KSEs and what is POSIX thread:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/linux_thread_process.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>POSIX thread, KSE, PID/TGID/TID relationship</em></td>
    </tr>
  </tbody>
</table>

<p>There some key points about the effect of this flag:</p>

<ul>
  <li>We can call KSEs created with CLONE_THREAD flag <code class="language-plaintext highlighter-rouge">threads</code></li>
  <li>No signals is sent to the calling KSE when <code class="language-plaintext highlighter-rouge">thread</code> is terminated, so <code class="language-plaintext highlighter-rouge">thread</code> can not be waited like <code class="language-plaintext highlighter-rouge">process</code>; the right way to <em>wait</em> a <code class="language-plaintext highlighter-rouge">thread</code> to terminate is throuth the <code class="language-plaintext highlighter-rouge">join()</code> semantics. The cornerstone behind the <code class="language-plaintext highlighter-rouge">join</code> is <em>futex</em>, which we dicuss in <a href="https://shan-weiqiang.github.io/2024/06/08/futex-syscall-foundation-for-mutex-and-semaphore.html">futex</a>. For how the <code class="language-plaintext highlighter-rouge">join</code> works and the behaviors of the <code class="language-plaintext highlighter-rouge">join</code>, i will write another blog. For now we need to konw that <code class="language-plaintext highlighter-rouge">thread</code> created with CLONE_THREAD can not be waited using <code class="language-plaintext highlighter-rouge">wait()/waitpid()</code>  and must use <code class="language-plaintext highlighter-rouge">join</code> to wait for it</li>
  <li>When all KSEs inside one TGID(PID) terminate, a SIGCHLD signal is sent to parent process of this TGID</li>
  <li>If any <code class="language-plaintext highlighter-rouge">thread</code> inside one thread group calls <code class="language-plaintext highlighter-rouge">exec()</code>, all other threads except for the thread group leader are terminated and the new program is executed inside the thread group leader</li>
  <li>If any <code class="language-plaintext highlighter-rouge">thread</code> inside one thread group calls <code class="language-plaintext highlighter-rouge">fork</code>, anyother <code class="language-plaintext highlighter-rouge">thread</code> inside this thread group can call <code class="language-plaintext highlighter-rouge">wait</code> on it
    <ul>
      <li><code class="language-plaintext highlighter-rouge">fork</code> only have something to do with the calling <code class="language-plaintext highlighter-rouge">thread</code>, except for the <code class="language-plaintext highlighter-rouge">wait</code> operation above, other <code class="language-plaintext highlighter-rouge">thread</code> does not have much to do with the forked process</li>
    </ul>
  </li>
  <li>If CLONE_THREAD is specified, CLONE_SIGHAND must be specifed, again CLONE_VM must be specifed</li>
</ul>

<h1 id="clone_parent_settidclone_child_settidclone_child_cleartid">CLONE_PARENT_SETTID/CLONE_CHILD_SETTID/CLONE_CHILD_CLEARTID</h1>

<p>Those are flags to support POSIX threads.</p>

<ul>
  <li>CLONE_PARENT_SETTID: <code class="language-plaintext highlighter-rouge">clone</code> will set the ID of the returned KSE to the parameter <code class="language-plaintext highlighter-rouge">pid_t *ptid</code>. The value is the same as the return value of <code class="language-plaintext highlighter-rouge">clone</code>.
    <ul>
      <li>The set of the value happens before the duplication of virtual memory, so even the CLONE_VM not specified, the child and parent both can see the newly created <code class="language-plaintext highlighter-rouge">pid</code>. CLONE_VM is required for POSIX threads</li>
      <li>Getting <code class="language-plaintext highlighter-rouge">pid</code> through parameter and through the return value is different: for example, if the returned KSE terminates immediately <em>before</em> the parent has the chance to do the assignment of the return value, and if the SIGCHLD handler in parent use the <code class="language-plaintext highlighter-rouge">pid</code>, the <code class="language-plaintext highlighter-rouge">pid</code> is invalid, because the <code class="language-plaintext highlighter-rouge">pid</code> has not been assigned yet. But if the <code class="language-plaintext highlighter-rouge">pid</code> is aquired by argument <code class="language-plaintext highlighter-rouge">ptid</code>, due to the fact that the write of the <code class="language-plaintext highlighter-rouge">pid</code> to <code class="language-plaintext highlighter-rouge">ptid</code> is done before the <code class="language-plaintext highlighter-rouge">clone</code> returns, the parent SIGCHLD handler can safely use this <code class="language-plaintext highlighter-rouge">pid</code></li>
    </ul>
  </li>
  <li>CLONE_CHILD_SETTID: <code class="language-plaintext highlighter-rouge">clone</code> write the ID of the newly created KSE into the child’s memory location specified by argument <code class="language-plaintext highlighter-rouge">pid_t *ctid</code>.  Note that if CLONE_VM is specified, this will also affect the parent. For POSIX threads, CLONE_VM must be specified. So for the POSIX thread implmentation, CLONE_PARENT_SETTID and CLONE_CHILD_SETTID  overlapps in functionality</li>
  <li>CLONE_CHILD_CLEARTID: <code class="language-plaintext highlighter-rouge">clone</code> zeros the memory pointed by <code class="language-plaintext highlighter-rouge">pid_t *ctid</code></li>
</ul>

<h2 id="pthread_join-under-the-hood">pthread_join under the hood</h2>

<p>In linux, the <code class="language-plaintext highlighter-rouge">pthread_join/pthread_create</code> is implemented based on these three flags. When <code class="language-plaintext highlighter-rouge">pthread_create</code> creates threads, CLONE_PARENT_SETTID and CLONE_CHILD_CLEARTID is used, <code class="language-plaintext highlighter-rouge">pid_t *ptid</code> and <code class="language-plaintext highlighter-rouge">pid_t *ctid</code> are set to point to the same location. CLONE_CHILD_SETTID is irrelevent because POSIX thread requires the CLONE_VM. Kernel does the following trick to support POSIX threads:</p>

<ul>
  <li>Kernel treat the memory pointed to by <code class="language-plaintext highlighter-rouge">pid_t *ptid</code> and <code class="language-plaintext highlighter-rouge">pid_t *ctid</code> as <a href="https://shan-weiqiang.github.io/2024/06/08/futex-syscall-foundation-for-mutex-and-semaphore.html">futex</a></li>
  <li>When <code class="language-plaintext highlighter-rouge">pthread_join</code> joins the <code class="language-plaintext highlighter-rouge">pid_t</code>, it actually <code class="language-plaintext highlighter-rouge">FUTEX_WAIT</code> on this <em>futex</em>, if condition is not met, then calling thread is put into block</li>
  <li>When the KSE terminates, since then CLONE_CHILD_CLEARTID is specified, the <em>futex</em> is cleared and <code class="language-plaintext highlighter-rouge">FUTEX_WAKE</code> is called on this <em>futex</em>, which wakes up thread that is waiting on this <em>futex</em>. This mechanism achieve the behavior that the <code class="language-plaintext highlighter-rouge">phread_join</code> calling thread is blocked until the termination of the joined thread denoted by <code class="language-plaintext highlighter-rouge">pid_t</code></li>
</ul>

<h1 id="clone_settls">CLONE_SETTLS</h1>

<p>If specified, the argument <code class="language-plaintext highlighter-rouge">user_desc *tls</code> is used as thread-local storage. This storage is only accesible by the newly created KSE</p>

<h1 id="use-clone-to-implement-fork-and-posix-threads">use <code class="language-plaintext highlighter-rouge">clone</code> to implement <code class="language-plaintext highlighter-rouge">fork</code> and POSIX threads</h1>

<p><code class="language-plaintext highlighter-rouge">fork</code> and POSIX threads can be implemented by <code class="language-plaintext highlighter-rouge">clone</code> with different flags specifed:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">fork</code> corresponds to flags combination: <code class="language-plaintext highlighter-rouge">CLONE_VM | CLONE_VFORK | SIGCHLD</code></li>
  <li>POSIX threads corresponds to flags combination: <code class="language-plaintext highlighter-rouge">CLONE_VM | CLONE_FILES | CLONE_FS | CLONE_SIGHAND | CLONE_THREAD | CLONE_SETTLS | CLONE_PARENT_SETTID | CLONE_CHILD_CLEARTID | CLONE_SYSVSEM</code></li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Demonstrate the use of the clone(..) to simulate fork and std::threads</span>
<span class="cp">#include</span> <span class="cpf">&lt;chrono&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;csignal&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cstddef&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;ctime&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;mutex&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;pthread.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;sched.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdlib.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;sys/wait.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thread&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;unistd.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
</span>
<span class="cp">#define STACK_SIZE 65536
</span>
<span class="kt">void</span> <span class="nf">sig_handler</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">{</span> <span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span> <span class="p">}</span>

<span class="c1">// mutex to synchronize printf</span>
<span class="n">std</span><span class="o">::</span><span class="n">mutex</span> <span class="n">mtx</span><span class="p">;</span>

<span class="c1">// entry function for clone(..)</span>
<span class="kt">int</span> <span class="nf">clone_func</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
    <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lck</span><span class="p">{</span><span class="n">mtx</span><span class="p">};</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"clone thread: "</span> <span class="o">&lt;&lt;</span> <span class="n">getpid</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">seconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// level two std::thread function, used to demonstrate that even they are</span>
<span class="c1">// created nested, they are peers with the thread that created them</span>
<span class="kt">void</span> <span class="nf">level_two</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
    <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lck</span><span class="p">{</span><span class="n">mtx</span><span class="p">};</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"POSIX thread, id: "</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">get_id</span><span class="p">()</span>
                <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">seconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// level one std::thread function</span>
<span class="kt">void</span> <span class="nf">level_one</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">t</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span><span class="n">level_two</span><span class="p">);</span>
  <span class="n">t</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
<span class="p">}</span>

<span class="n">pid_t</span> <span class="n">child_pid</span><span class="p">;</span>
<span class="n">pid_t</span> <span class="n">parent_pid</span><span class="p">;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>

  <span class="n">std</span><span class="o">::</span><span class="n">signal</span><span class="p">(</span><span class="n">SIGINT</span><span class="p">,</span> <span class="n">sig_handler</span><span class="p">);</span>

  <span class="c1">// Stack for the new thread</span>
  <span class="kt">char</span> <span class="o">*</span><span class="n">stack</span><span class="p">;</span>

  <span class="c1">// Top of the stack</span>
  <span class="kt">char</span> <span class="o">*</span><span class="n">stackTop</span><span class="p">;</span>
  <span class="n">pid_t</span> <span class="n">pid</span><span class="p">;</span>

  <span class="c1">// Allocate memory for the stack</span>
  <span class="n">stack</span> <span class="o">=</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">STACK_SIZE</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">stack</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">exit</span><span class="p">(</span><span class="n">EXIT_FAILURE</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="c1">// Calculate the top of the stack</span>
  <span class="n">stackTop</span> <span class="o">=</span> <span class="n">stack</span> <span class="o">+</span> <span class="n">STACK_SIZE</span><span class="p">;</span>

  <span class="c1">// use `ps --pid &lt;pid&gt; -O tid,lwp,nlwp -L` to see the difference</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// CLONE_THREAD flag prevent from creating new thread group ID(the same as</span>
    <span class="c1">// process ID); this thread will be peers to threads that are created by</span>
    <span class="c1">// std::thread</span>
    <span class="c1">// emulate the POSIX threads, like std::thread</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">clone_func</span><span class="p">,</span> <span class="n">stackTop</span><span class="p">,</span>
                <span class="n">CLONE_VM</span> <span class="o">|</span> <span class="n">CLONE_FILES</span> <span class="o">|</span> <span class="n">CLONE_FS</span> <span class="o">|</span> <span class="n">CLONE_SIGHAND</span> <span class="o">|</span>
                    <span class="n">CLONE_THREAD</span> <span class="o">|</span> <span class="n">CLONE_SETTLS</span> <span class="o">|</span> <span class="n">CLONE_PARENT_SETTID</span> <span class="o">|</span>
                    <span class="n">CLONE_CHILD_CLEARTID</span> <span class="o">|</span> <span class="n">CLONE_SYSVSEM</span><span class="p">,</span>
                <span class="o">&amp;</span><span class="n">parent_pid</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">child_pid</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">pid</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">exit</span><span class="p">(</span><span class="n">EXIT_FAILURE</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="c1">// create new thread group ID, aka creating new process ID</span>
    <span class="c1">// emulate fork</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">clone_func</span><span class="p">,</span> <span class="n">stackTop</span><span class="p">,</span> <span class="n">SIGCHLD</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">pid</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">exit</span><span class="p">(</span><span class="n">EXIT_FAILURE</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lck</span><span class="p">{</span><span class="n">mtx</span><span class="p">};</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Parent process: Created child thread with PID = %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">pid</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Parent process: PID = %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">getpid</span><span class="p">());</span>
  <span class="p">}</span>

  <span class="c1">// standard POSIX comforming threads</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="o">&gt;</span> <span class="n">threads</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">threads</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span><span class="n">level_one</span><span class="p">));</span>
  <span class="p">}</span>

  <span class="c1">// wait for signals</span>
  <span class="n">pause</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="linux-programming" /><summary type="html"><![CDATA[This blog reference most of it’s content to Chapter 28.2 of The Linux Programming Interface]]></summary></entry><entry><title type="html">reference count in computer systems</title><link href="http://localhost:4000/2024/06/23/reference-count-usage.html" rel="alternate" type="text/html" title="reference count in computer systems" /><published>2024-06-23T15:20:46+08:00</published><updated>2024-06-23T15:20:46+08:00</updated><id>http://localhost:4000/2024/06/23/reference-count-usage</id><content type="html" xml:base="http://localhost:4000/2024/06/23/reference-count-usage.html"><![CDATA[<p>reference count is a recurring design pattern in many fields of computer systems. It’s interesting to put them together and have a look at them.</p>

<p>If not otherwise specified, all pictures used in this blog comes from the book: <a href="https://man7.org/tlpi/">The Linux Programming Interface</a>.</p>

<h1 id="i-node-reference-counthard-links">i-node reference count:hard links</h1>

<p>let’s first have a recall about how a file is stored in disk <em>logically</em>. we don’t go into details about how file is <em>physically</em> stored in disk, which is the job of the disk controller. From the point of the operating system, the disk is composed of continuous <em>logical blocks</em>. A disk might have been partitioned into several <em>partition</em>s, every partition can contain a individual <em>file system</em>. Following discussion is based on the <em>ext2</em> file system.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/disk_partition.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>The position of i-node</em></td>
    </tr>
  </tbody>
</table>

<p>The i-node table is the <em>index</em> of all the files that resides in this partition(aka, file system). i-node table contains <em>i-node entry</em>. An i-node entry defines a <em>file</em> inside the file system, here a <em>file</em> can be a regular file, a directory, a symbol link. The i-node entry contains basically all the meta information about the file: type of the file, creation time, access settings, logical block numbers that store the content of the file, etc. Note that the logical block numbers are continuous inside the i-node entry, but each logical block number can point to any logical block, which implies:</p>

<ul>
  <li>even though the logical blocks are continuous from the perspective of the operating system, the file system stores each file discretely among all logical blocks</li>
  <li>logical blocks themselves are again allocated discretely among all physical blocks by the disk controller</li>
</ul>

<p>As the result of those two level indirections, a file can be concieved as being stored discretely in a disk. Before going into our main topic about the reference count, it’s worth to know about how an i-node entry is structured. No matter the size of a file, the i-node entry for the file has a fixed size. This, again, is achieved by indirection.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/i_node.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>i-node internal structure</em></td>
    </tr>
  </tbody>
</table>

<p>Note that every pointer is to a logical block, and suppose that a logical block is 1024-bytes, a pointer is 4-bytes, there can be a lot of blocks referenced by a single i-node entry. That is why one i-node entry can contain information about very large files.</p>

<p>A folder, like a regular file, is also represented as an i-node entry inside i-node table. Folders also need logical blocks to stores it’s content, the difference between folders and regular files lies in the content. For a regular file, the content stored in logical blocks are user data; For folders, the content are key-value pairs that describe the <em>files</em> inside the folder. The key is a string representing the file name; The value is the i-node number of this file.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/file_path.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>file path</em></td>
    </tr>
  </tbody>
</table>

<p>When we try to open a file, such as “/a/b/c”, we can imagine the following steps:</p>

<ol>
  <li>”/” folder i-node entry is iterated to look for the i-node corresponding to file name “a”</li>
  <li>iterate i-node found in step 1 to look for the i-node corresponding to file name “b”</li>
  <li>repeat above steps until file name “c” is found, together with the i-node number</li>
  <li>from the i-node number, retrieve the logical blocks numbers that are used to store file “c”, and read content from those logical blocks by interacting with disk controller</li>
</ol>

<p>Finally we come to the point of our topic, the reference counting. It’s very intuitive to ask that what if two file names in two difference folders both point to the same i-node entry?  An i-node can be referenced by multiple <em>hard link</em>s. For example, “/a/b/c” and “/e/f/g” might both point to the same i-node. Linux syscall <code class="language-plaintext highlighter-rouge">link(..)</code> and <code class="language-plaintext highlighter-rouge">unlink(..)</code> are used to create and remove a hard link to a specific i-node entry. When new hard link is created, the reference count for the i-node is incremented. When hard link is removed for the i-node, the reference count is decremented. When the reference count is reached 0, the i-node entry and the logical blocks that are associated with this i-node are all freed by the file system(also require that all file descriptors that refer to this i-node are all closed).</p>

<h1 id="open-file-description-reference-count">open file description reference count</h1>

<p>When a file is opened, the corresponding i-node is searched and loaded by the file system. There is one <em>file description</em> for this <em>open</em> action. If the same file is opened multiple times, there will be multiple file descriptions. If different hard links that link to the same i-node are opened, there are multiple file descriptions that point to the same i-node. The opened file description is system-wide, meaning that it might be shared by different file descriptors. Those descriptors might be in the same process, or they can be in different process. Different file descriptors refering to the same file description in the same process might be created with <code class="language-plaintext highlighter-rouge">dup</code>, <code class="language-plaintext highlighter-rouge">dup2</code> system call; Different file descriptors refering to the same file description in different process might be created with <code class="language-plaintext highlighter-rouge">fork</code> system call. The same file description shares read/write offset and status, etc, because the file description is opened once.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/file_description.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>file descriptor, file description and i-node relationship</em></td>
    </tr>
  </tbody>
</table>

<p>There is a reference count for every file description, one file descriptor adds one count into a file description. When all the file descriptors refering to the file description are closed, the file description is closed by the file system.</p>

<h1 id="garbage-collection-and-c-smart-pointers">garbage collection and c++ smart pointers</h1>

<p>Reference counting is also used by many other fields, amoung them are garbage collection for programming languages and c++ shared pointer design. In programming languages that support garbage collection, every variable created in program is counted by code that are automatically generated by the compiler, when one reference to this variable is going out of scope, the reference count is decremented. When the reference count is reduced to 0, the variable is collected as <em>garbage</em>, meaning that the memory occupied by this variable is returned to the kernel. The design of the c++ shared pointer is very similar to that of the garbage collector,except that the reference counting is implemented by programmer instead of the compiler.</p>]]></content><author><name></name></author><category term="linux-programming" /><summary type="html"><![CDATA[reference count is a recurring design pattern in many fields of computer systems. It’s interesting to put them together and have a look at them.]]></summary></entry><entry><title type="html">the mapping design pattern in storage management</title><link href="http://localhost:4000/2024/06/09/the-mapping-design-pattern-in-computer-systems.html" rel="alternate" type="text/html" title="the mapping design pattern in storage management" /><published>2024-06-09T09:20:46+08:00</published><updated>2024-06-09T09:20:46+08:00</updated><id>http://localhost:4000/2024/06/09/the-mapping-design-pattern-in-computer-systems</id><content type="html" xml:base="http://localhost:4000/2024/06/09/the-mapping-design-pattern-in-computer-systems.html"><![CDATA[<p>It’s fun to observe and generalize the similarities between different systems. In computer storage, the virtual memory design in RAM management and the logical block design in mass storage management shares the same pattern: mapping of logical/virtual memory to physical memory.</p>

<p>Reference: <a href="https://americas.kioxia.com/content/dam/kioxia/en-us/business/memory/mlc-nand/asset/KIOXIA_Managed_Flash_BOS_P5_Understanding_L2P_Block_Address_Translation_Tech_Brief.pdf">Managed Flash Background Operations Series</a></p>

<h2 id="virtual-address-to-physical-address-mapping-in-ram-management">virtual address to physical address mapping in RAM management</h2>

<p>In order for a process to have <em>continuous</em> memory address and to have bigger address space than actual size of the RAM, virtual memory is invented. We are not focusing on the techniques about how virtual memory works here, instead we are comparing similarities. Following points need to be noted:</p>

<ul>
  <li>kernel manages processes and their virtual tables, which mapps virtual memory to physical memories</li>
  <li>kernel manages physical memories and is responsible for the algorithm for how the physical memory is used, such as which page is swapped out(page replacement algorithm); and how many physical memory is allocated to each process（allocation algorithm）</li>
  <li>user program decides, at runtime, which physical memory will be accessed</li>
  <li>user program have <em>continuous</em> address space</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/virtual_memory.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Operating system concepts / Abraham Silberschatz</em></td>
    </tr>
  </tbody>
</table>

<h2 id="logical-block-to-physical-block-mapping-in-mass-storage-management">logical block to physical block mapping in mass storage management</h2>

<p>In order for the kernel to have <em>continuous</em> storage block access, the storage controller maintains a logical block address to physical block address mapping table. Following points need to be noted:</p>

<ul>
  <li>storage device controller mantains the mapping table from logical block address to physical block address</li>
  <li>kernel use the logical block address as continuous blocks</li>
  <li>storage device is responsible for allocation and reallocation a logical block to a physical block</li>
  <li>kernel decides when to use the logical block, such as when creating files, and is reponsible for optimization for accessing logical blocks, for example, kernel might use algorithm to read adjacent logical blocks sequentially so that the seek operation is more efficient for the device controller; kernel is also responsible to reduce fragmentation in the use of the logical address space</li>
  <li>storage device is responsible for garbage collection and wear leveling of the physical blocks and might assign different physical block address for the same logical block(kernel knows nothing about this)</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/logical_block1.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Logical block addressing and metadata mapping within NAND flash memory</em></td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/logical_block2.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Logical block addresses seen by the host do not change even after the physical block address space changed</em></td>
    </tr>
  </tbody>
</table>

<h2 id="philosophy-of-detaching-physical-from-logical">philosophy of detaching physical from logical</h2>

<p>The ideology behind virtual memory design and the logical block address design is the same: detach logical memory from physical memory, through the use of mapping table:</p>

<ul>
  <li>physical memory can be mapped to multi logical/virtual memory, thus shared by them</li>
  <li>logical/virtual memory is continuous, while the actual storage in physical memory is not. This is a huge advantage, since physical memory unit might be corrupted. With the help of mapping table, the corrupted memory unit can be remapped to a good one and the logical memory need not to know anything about it</li>
</ul>

<p>It is indeed true that：</p>

<blockquote>
  <p>“We can solve any problem by introducing an extra level of indirection.”</p>
</blockquote>]]></content><author><name></name></author><category term="operating-system" /><summary type="html"><![CDATA[It’s fun to observe and generalize the similarities between different systems. In computer storage, the virtual memory design in RAM management and the logical block design in mass storage management shares the same pattern: mapping of logical/virtual memory to physical memory.]]></summary></entry><entry><title type="html">futex: foundation of linux synchronization</title><link href="http://localhost:4000/2024/06/08/futex-syscall-foundation-for-mutex-and-semaphore.html" rel="alternate" type="text/html" title="futex: foundation of linux synchronization" /><published>2024-06-08T19:20:46+08:00</published><updated>2024-06-08T19:20:46+08:00</updated><id>http://localhost:4000/2024/06/08/futex-syscall-foundation-for-mutex-and-semaphore</id><content type="html" xml:base="http://localhost:4000/2024/06/08/futex-syscall-foundation-for-mutex-and-semaphore.html"><![CDATA[<p>In linux, pthread mutex <code class="language-plaintext highlighter-rouge">pthread_mutex_t</code>(on which <code class="language-plaintext highlighter-rouge">std::mutex</code> is based),  and <code class="language-plaintext highlighter-rouge">pthread_cond_t</code>(on which <code class="language-plaintext highlighter-rouge">std::condition_variable</code> is based), and semaphores, all use futex kernel support as their internal implementation. Let’t dig into what is futex and the behavior of it.</p>

<p>Reference: 
<a href="https://www.man7.org/linux/man-pages/man2/futex.2.html">futex</a>
<a href="https://man7.org/linux/man-pages/man2/syscalls.2.html">syscall</a></p>

<h2 id="futex-behavior">futex behavior</h2>

<p>futex is called fast user-space locking. At first glampse it’s confusing since futex is a syscall with following signature:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">long</span> <span class="nf">syscall</span><span class="p">(</span><span class="n">SYS_futex</span><span class="p">,</span> <span class="kt">uint32_t</span> <span class="o">*</span><span class="n">uaddr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">futex_op</span><span class="p">,</span> <span class="kt">uint32_t</span> <span class="n">val</span><span class="p">,</span>
                    <span class="k">const</span> <span class="k">struct</span> <span class="n">timespec</span> <span class="o">*</span><span class="n">timeout</span><span class="p">,</span>   <span class="cm">/* or: uint32_t val2 */</span>
                    <span class="kt">uint32_t</span> <span class="o">*</span><span class="n">uaddr2</span><span class="p">,</span> <span class="kt">uint32_t</span> <span class="n">val3</span><span class="p">);</span>
</code></pre></div></div>

<p>Why it is called <em>user-space</em> locking when it needs kernel support? The answer is the second parameter <code class="language-plaintext highlighter-rouge">uaddr</code>, which is a user space 32-bit word. <code class="language-plaintext highlighter-rouge">uaddr</code> is a user space address inside the calling process, it serves two purposes:</p>

<ul>
  <li>threads do compare-and-swap operations on this integer to change the value, in user mode; threads do futex syscall operations through this address, which <strong>is used to connect the synchronization in user space with the implementation of blocking by the kernel</strong>.</li>
  <li>kernel transform this userspace <code class="language-plaintext highlighter-rouge">uaddr</code> into a unique identifier and use it to maintain a unique threads waiting list; kernel accept user space syscalls which pass <code class="language-plaintext highlighter-rouge">uaddr</code> into kernel space, then kernel does corresponding operations, such as put thread into waiting list, or wake up specific number of threads in the waiting list. There are mainly two operations the kernel does:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">FUTEX_WAIT</code>: put the calling thread into the waiting list uniquely identified by <code class="language-plaintext highlighter-rouge">uaddr</code>, if and only if the value in <code class="language-plaintext highlighter-rouge">uaddr</code> is equal to <code class="language-plaintext highlighter-rouge">val</code>; if <code class="language-plaintext highlighter-rouge">uaddr</code> is not equal to <code class="language-plaintext highlighter-rouge">val</code>, the syscall returns immediately. Above operations are done atomically.</li>
      <li><code class="language-plaintext highlighter-rouge">FUTEX_WAKE</code>: wake up <code class="language-plaintext highlighter-rouge">val</code> number of threads in the waiting list uniquely identified by <code class="language-plaintext highlighter-rouge">uaddr</code>. There is no operations on the value in <code class="language-plaintext highlighter-rouge">uaddr</code></li>
    </ul>
  </li>
</ul>

<p>The functionality futex provides is basically a threads queue and dequeue mechanism. Even though mutex, condition variable, semaphore are all implemented on top of futex, the kernel knows nothing about the real usage of these concept. What the kernel knows is when to put a thread into a unique waiting list and when to remove a thread from a unique waiting list, all of which are provoked by user. Kernel provides a <em>service</em>, it’s up to user for how to used it and what it is used for.</p>

<p>It’s useful to rethink about what exactly is a lock. At it’s simplest form, lock is a binary flag, 0 or 1. What is special about locks is that they should be operated atomically. At user space, atomicity can be achieved through compare-and-swap operation. So the spinlock can be implemented without any kernel support. However, what if we want to put thread into waiting state if it currently can not get the lock to optimize the CPU resources? How to wake up threads when lock is free to use? These scheduling of threads need kernel support. futex is in the position to fulfil these needs.</p>

<p>Take the simplest binary lock for example, if one thread is competing for the lock, what it should do is to firstly do atomic operation to decide whether the lock is free, if it’s free, the binary is flipped and the lock is aquired by this specific thread. Please note that during this process, no kernel support is needed. Since the switch from user mode to kernel mode is a costly action, the user space fast locking functionality provided by futex is indeed <em>fast</em>. If unluckily, the lock is not available after the atomic check operation, a futex syscall is required to put the calling thread into waiting list for this lock, this is when a costly kernel mode switching is needed.</p>

<h2 id="intra-process-and-inter-process-synchronization">intra-process and inter-process synchronization</h2>

<p>From the perspective of a process the futex word is in the address space of current process. If two threads come from this same process, it is obviously no problem if the same futex word is used, since they shared the futex in the same address space. futex word can be a global variable in this scenario.</p>

<p>What if threads from two different process want to do synchronization using futex? It’s intuitive to think that the futex word should be in a shared memory region. This is indeed the solution. If the futex word is in a shared memory region from different processes, <em>inter-process</em> synchronization can be achieved. It is rather easy from the user side, since what the user need to do is simply create a shared memory region and put the futex word in there and share it across processes. The usage of the futex is totally the same as in non-shared memory scenario. However, the kernel now has a headache to solve: how to uniquely identify inter-process futex？</p>

<p>In intra-process scenario, the kernel can use PID plus the virtual address of the futex word to uniquely identify a futex inside the kernel. In inter-process scenario, this is invalid since the futex word have different virtual address in each processes. The requirement is that kernel need to find a way to uniquely identify a futex, even across different virtual address space. One possible solution is to use the physical address to uniquely identify the futex, since the physical address is the same across all processes in this situation. However, the futex might be swapped out of RAM during runtime and every kernel have it’s own page replacement algorithm. After the memory region containing the futex is swapped in, it might have a different physical address with the previous one. The kernel might be implemented that the page that contains any futex word must not be swapped out. We wont’ go further about the kernel implementations here, but at the end, for a inter-process futex to work, it’s kernel’s job to ensure:</p>

<ul>
  <li>every process that use this futex must share the same waiting list for blocking thread</li>
  <li>all processes’s futex syscall works atomically on this futex word inside the kernel(user space operation on this futex word is assured by user)</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">pthread_mutex_t</code> and named semaphores in POSIX can be used to achieve inter-process synchronization. The C++ <code class="language-plaintext highlighter-rouge">std::mutex</code> is only designed for intra-process usage.</p>]]></content><author><name></name></author><category term="linux-programming" /><summary type="html"><![CDATA[In linux, pthread mutex pthread_mutex_t(on which std::mutex is based), and pthread_cond_t(on which std::condition_variable is based), and semaphores, all use futex kernel support as their internal implementation. Let’t dig into what is futex and the behavior of it.]]></summary></entry><entry><title type="html">AUTOSAR service model: a big picture</title><link href="http://localhost:4000/2024/06/08/AUTOSAR-Service-Model-a-big-picture.html" rel="alternate" type="text/html" title="AUTOSAR service model: a big picture" /><published>2024-06-08T09:22:46+08:00</published><updated>2024-06-08T09:22:46+08:00</updated><id>http://localhost:4000/2024/06/08/AUTOSAR-Service-Model-a-big-picture</id><content type="html" xml:base="http://localhost:4000/2024/06/08/AUTOSAR-Service-Model-a-big-picture.html"><![CDATA[<p>The design phase of AUTOSAR communication management can be generalized as following steps: First, a <em>methodology</em> is chosen. According to the methodology, <em>standards</em> and <em>protocols</em> are designed, specifying detailed <em>behavior</em>. AUTOSAR vendors and open source organizations <em>implement</em> these <em>standards</em> and <em>protocol</em>.</p>

<p>AUTOSAR Service Model is based on SOA(service-oriented architecture). According to this service model, AUTOSAR designs the user level API, aka the ara::com::API and the low level communication protocol SOME/IP. ara::com::API and SOME/IP both inherit concept from AUTOSAR service model, so they both have similar concept like service,event, method, etc, even though the meaning of these names varies in its own context. ara::com API can be implemented on other communication protocols, like DDS, instead of SOME/IP. (Interestingly, DDS is not service-oriented.).</p>

<p>These steps can be illustrated in the following diagram:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/autosar_service_model.png" alt="Alt text" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>AUTOSAR Service Model</em></td>
    </tr>
  </tbody>
</table>

<p>The soul of the software is the AUTOSAR service model, which defines <em>service description</em>:</p>

<ul>
  <li>A service is interface between service provider and service consumer(SOA)</li>
  <li>AUTOSAR service consists of one or more of following elements:
    <ul>
      <li>events</li>
      <li>methods</li>
      <li>fields</li>
    </ul>
  </li>
</ul>

<p>ara::com API and SOME/IP both contain these concepts, but they describe them in different levels:</p>

<ul>
  <li>SOME/IP is low level <em>communication oriented</em> protocol. It emphasize the <em>transportation</em> of <em>data</em>. It doesn’t care about method calls, event handling. It’s job is to compose corresponding messages and deliver them to desired target.</li>
  <li>ara::com API is high level <em>user oriented</em> standards. It focus on the user side and give detailed requirement on <em>behavior</em> of these concepts. For example, ara::com API defines fixed class, method signature and data structures to unify the <em>behavior</em> in the eye of the user. ara::com API defines how service provider implement their method logic, how consumer implement their event handler. ara::com API also specifies the execution mechanism, polling or event-driven, for method and event handling. Except for events, methods, fields, ara::com API extends another element: triggers, which does not appear in AUTOSAR service model and SOME/IP.</li>
</ul>

<p>Implementation details can be very different for SOME/IP, as long as the implementation adhere to the message format. Implemetentations for ara::com API must use the standard code specified by AUTOSAR, so user does not need to care about the underlying details. The underlying implementation for the ara::com API is called <em>binding</em>, and the underlying protocol used for communication is called <em>driver</em> or <em>communication driver</em>. ara::com API can use SOME/IP as <em>driver</em>, which is the AUTOSAR standard. Besides SOME/IP, the ara::com API can also be bound to DDS, whose binding requirement are included in standard.</p>]]></content><author><name></name></author><category term="autosar" /><summary type="html"><![CDATA[The design phase of AUTOSAR communication management can be generalized as following steps: First, a methodology is chosen. According to the methodology, standards and protocols are designed, specifying detailed behavior. AUTOSAR vendors and open source organizations implement these standards and protocol.]]></summary></entry><entry><title type="html">ara::com API解析[Part 3]</title><link href="http://localhost:4000/2024/06/01/ara-com-API-%E8%A7%A3%E8%AF%BB-Part-3.html" rel="alternate" type="text/html" title="ara::com API解析[Part 3]" /><published>2024-06-01T09:22:46+08:00</published><updated>2024-06-01T09:22:46+08:00</updated><id>http://localhost:4000/2024/06/01/ara::com-API-%E8%A7%A3%E8%AF%BB-Part-3</id><content type="html" xml:base="http://localhost:4000/2024/06/01/ara-com-API-%E8%A7%A3%E8%AF%BB-Part-3.html"><![CDATA[<p>标准连接：<a href="https://www.autosar.org/fileadmin/standards/R23-11/AP/AUTOSAR_AP_EXP_ARAComAPI.pdf">ara::com API</a></p>

<hr />
<p><strong>NOTE</strong></p>

<p>以下文中用到的词语解释：</p>

<ul>
  <li><em>通信协议</em>，<em>驱动协议</em>： 如果不是特别说明，这里<em>通信协议</em>指的是ComAPI底层使用的通信驱动协议，例如DDS、SOME/IP。注意<strong>不是</strong>网络协议</li>
  <li><em>ComAPI</em>, <em>ComAPI绑定</em>，<em>绑定层</em>，<em>绑定</em>，<em>适配层</em>：这些都是指ComAPI层的接口实现，也称作绑定实现；它是存在于应用层与通信协议之间的适配代码</li>
  <li><em>Proxy</em>，<em>Proxy类</em>，<em>客户端</em>：如无特别说明，都是指服务的消费方，Proxy接收Skeleton提供的服务</li>
  <li><em>Skeleton</em>，<em>Skeleton类</em>，<em>服务端</em>：如无特别说明，都是指服务提供方，Skeleton为Proxy提供服务</li>
  <li><em>服务实例</em>，<em>Skeleton类实例</em>，<em>Skeleton实例</em>：都是指一个具体的服务实例， 它是Skeleton类的实例；服务实例全局唯一</li>
  <li><em>消费实例</em>，<em>Proxy类实例</em>， <em>Proxy实例</em>：是指一个具体的消费方，它有唯一的Client ID; 注意Proxy实例是没有服务实例ID的，服务ID是Skeleton实例的唯一标识符</li>
  <li>Skeleton/Proxy的<strong>实例</strong>与Seleton/Proxy<strong>类</strong>是两个概念，文中如果指的是实例，则会具体写明，否则指的是Skeleton/Proxy类本身</li>
</ul>

<hr />

<ul id="markdown-toc">
  <li><a href="#536-methods" id="markdown-toc-536-methods">5.3.6 Methods</a>    <ul>
      <li><a href="#5361-one-way-aka-fire-and-forget-methods" id="markdown-toc-5361-one-way-aka-fire-and-forget-methods">5.3.6.1 One-Way aka Fire-and-Forget Methods</a></li>
      <li><a href="#5362-event-driven-vs-polling-access-to-method-results" id="markdown-toc-5362-event-driven-vs-polling-access-to-method-results">5.3.6.2 Event-Driven vs Polling access to method results</a></li>
      <li><a href="#5363-canceling-method-result" id="markdown-toc-5363-canceling-method-result">5.3.6.3 Canceling Method Result</a></li>
    </ul>
  </li>
  <li><a href="#537-fields" id="markdown-toc-537-fields">5.3.7 Fields</a></li>
  <li><a href="#538-triggers" id="markdown-toc-538-triggers">5.3.8 Triggers</a></li>
  <li><a href="#54-skeleton-class" id="markdown-toc-54-skeleton-class">5.4 Skeleton Class</a>    <ul>
      <li><a href="#543-instantiation-constructors" id="markdown-toc-543-instantiation-constructors">5.4.3 Instantiation (Constructors)</a></li>
      <li><a href="#544-offering-service-instance" id="markdown-toc-544-offering-service-instance">5.4.4 Offering Service instance</a></li>
      <li><a href="#545-polling-and-event-driven-processing-modes" id="markdown-toc-545-polling-and-event-driven-processing-modes">5.4.5 Polling and event-driven processing modes</a>        <ul>
          <li><a href="#5451-polling-mode" id="markdown-toc-5451-polling-mode">5.4.5.1 Polling Mode</a></li>
          <li><a href="#5452-event-driven-mode" id="markdown-toc-5452-event-driven-mode">5.4.5.2 Event-Driven Mode</a></li>
        </ul>
      </li>
      <li><a href="#546-methods" id="markdown-toc-546-methods">5.4.6 Methods</a></li>
      <li><a href="#547-events" id="markdown-toc-547-events">5.4.7 Events</a></li>
    </ul>
  </li>
</ul>

<h3 id="536-methods">5.3.6 Methods</h3>

<blockquote>
  <p>The operator contains all of the service methods IN-/INOUT-parameters as INparameters. That means INOUT-parameters in the abstract service method description
are split in a pair of IN and OUT parameters in the ara::com API.</p>
</blockquote>

<hr />
<p><strong>NOTE</strong></p>

<p>服务接口中的每一个method都对应一个实现了<code class="language-plaintext highlighter-rouge">()</code>操作符的类。<code class="language-plaintext highlighter-rouge">()</code>操作符的入参就是method的In+In/Out参数；返回值是<code class="language-plaintext highlighter-rouge">ara::core::Future</code>类型，其模板参数类型是一个封装了这个method所有的Out+In/Out类型的结构体。关于In/Out类型，AUTOSAR规定要将其分别放在入参和返回值中，而不是通过非常量引用或者指针形式传入参数，然后通过引用或者指针原地返回结果。也就是说In/Out method参数类型，只是一个语法糖，等同于分别在In,Out中分别增加一个相同类型的参数。不支持通过引用或者指针原地返回结果的原因也很简单：method的返回类型是一个<code class="language-plaintext highlighter-rouge">future</code>，且必须马上返回，而不是block等待；如果入参是一个非常量引用或者指针，则method的调用依赖于周围的环境，则必须block等待，失去了method调用的异步特性。</p>

<hr />

<h4 id="5361-one-way-aka-fire-and-forget-methods">5.3.6.1 One-Way aka Fire-and-Forget Methods</h4>

<p>fire-and-forget模式返回类型为<code class="language-plaintext highlighter-rouge">void</code>，但是返回<code class="language-plaintext highlighter-rouge">void</code>的方法不一定是fire-and-forget:</p>

<ul>
  <li>fire-and-forget method的Proxy端不会等待Skeleton端的返回结果，而是在讲数据发送到网络后直接在本地的<code class="language-plaintext highlighter-rouge">promise</code>中<code class="language-plaintext highlighter-rouge">set_value</code></li>
  <li>返回值为<code class="language-plaintext highlighter-rouge">void</code>但不是fire-and-forget的method，即便返回值是<code class="language-plaintext highlighter-rouge">void</code>，Proxy端也要在接收到Skeleton端的通知后才会讲<code class="language-plaintext highlighter-rouge">promise</code>设置值，即便这个值是<code class="language-plaintext highlighter-rouge">void</code></li>
</ul>

<h4 id="5362-event-driven-vs-polling-access-to-method-results">5.3.6.2 Event-Driven vs Polling access to method results</h4>

<p>AUTOSAR method对事件event-driven和polling的支持是通过对<a href="https://en.cppreference.com/w/cpp/thread/future">std::future</a>的扩展实现的:</p>

<ul>
  <li>event-driven: AUTOSAR的<code class="language-plaintext highlighter-rouge">ara::core::Future</code>实现了如下接口，使<code class="language-plaintext highlighter-rouge">future</code>可以注册一个callback，当数据可用时直接异步回调</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">F</span><span class="p">&gt;</span>
<span class="k">auto</span> <span class="n">then</span><span class="p">(</span><span class="n">F</span><span class="o">&amp;&amp;</span> <span class="n">func</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span><span class="o">&lt;</span><span class="n">SEE_COMMENT_ABOVE</span><span class="o">&gt;</span><span class="p">;</span>
</code></pre></div></div>
<p>这个callback被调用的线程可能是<code class="language-plaintext highlighter-rouge">then</code>被调用的线程；也可能是<code class="language-plaintext highlighter-rouge">promise</code>的<code class="language-plaintext highlighter-rouge">set_value</code>线程</p>

<ul>
  <li>polling: AUTOSAR的<code class="language-plaintext highlighter-rouge">ara::core::Future</code>提供<code class="language-plaintext highlighter-rouge">bool is_ready() const;</code>，这个接口是non-blocking的，如果返回<code class="language-plaintext highlighter-rouge">true</code>，则<code class="language-plaintext highlighter-rouge">wait</code>肯定不会block</li>
</ul>

<h4 id="5363-canceling-method-result">5.3.6.3 Canceling Method Result</h4>

<p><code class="language-plaintext highlighter-rouge">promise/future</code> 是 <em>DefaultContructible</em> 和 <em>MoveConstructible</em>, 但不是<em>CopyConstructible</em> 或者 <em>CopyAssignable</em>的，所以如果想要放弃method结果，需要使用拷贝复制给<code class="language-plaintext highlighter-rouge">future</code>一个空的<code class="language-plaintext highlighter-rouge">ara::com::Future</code>，这样当<code class="language-plaintext highlighter-rouge">promise/future</code>的<code class="language-plaintext highlighter-rouge">shared state</code>就会只有<code class="language-plaintext highlighter-rouge">promise</code>的引用，当<code class="language-plaintext highlighter-rouge">promise</code>调用了<code class="language-plaintext highlighter-rouge">set_value</code>且生命周期结束后，<code class="language-plaintext highlighter-rouge">shared state</code>会自动释放。</p>

<h3 id="537-fields">5.3.7 Fields</h3>

<p>fields是event和method的集合，可以用如下几条总结fields的特点:</p>

<ul>
  <li>与event不同的时，一旦Proxy订阅了一个field，Skeleton会自动发送当前的值给Proxy</li>
  <li><code class="language-plaintext highlighter-rouge">Get()/Set()</code>可以用来获取或者设定当前的field值，就是普通的method</li>
</ul>

<p>除此之外，field与event一样，也有<code class="language-plaintext highlighter-rouge">Subscribe</code>,<code class="language-plaintext highlighter-rouge">GetSubscriptionState</code>,<code class="language-plaintext highlighter-rouge">SetReceiveHandler</code>等方法。</p>

<h3 id="538-triggers">5.3.8 Triggers</h3>

<p>trigger是一个特殊的event，它没有数据，所以不需要<code class="language-plaintext highlighter-rouge">local cache</code>。除了<code class="language-plaintext highlighter-rouge">GetNewSamples</code>方法变成<code class="language-plaintext highlighter-rouge">size_t GetNewTriggers()</code>以外，其他都与event相同。</p>

<h2 id="54-skeleton-class">5.4 Skeleton Class</h2>

<h3 id="543-instantiation-constructors">5.4.3 Instantiation (Constructors)</h3>

<blockquote>
  <p>Exactly for this reason the skeleton class (just like the proxy class) does neither support
copy construction nor copy assignment! Otherwise two “identical” instances would exist
for some time with the same instance identifier and routing of method calls would be
non-deterministic.</p>
</blockquote>

<hr />
<p><strong>NOTE</strong></p>

<p>每一个Skeleton和Proxy的<em>实例</em>都是全局唯一的，所以Skeleton和Proxy类都不是<em>CopyConstructible</em>和<em>CopyAssignable</em>的:</p>

<ul>
  <li>Skeleton的实例通过Instance ID唯一确定，全局唯一</li>
  <li>Proxy通过Skeleton的Instance ID实例化，与对应的Skeleton创建链接</li>
</ul>

<p>面对不同通信协议，binding层需要根据各个协议的特点适配，从而在ComAPI层看起来有一致的行为：</p>

<ul>
  <li>Skeleton的实例必须是全局唯一的，且能够接收不同Proxy实例的连接，对它们提供服务</li>
  <li>Proxy的实例必须能够与Skeleton进行一对一的通信，且要保证event订阅能够准确发送到Proxy实例；method的response能够和request准确配对，不能出现错乱</li>
</ul>

<hr />

<p>Skeleton有三种类型的构造函数，分别接收：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ara::com::InstanceIdentifier</code>: 一个确定的服务Instance, 需要包含ID，底层通信协议等信息；Skeleton类可以通过它来实例化底层的通信协议</li>
  <li><code class="language-plaintext highlighter-rouge">ara::com::InstanceIdentifierContainer</code>: 一组服务Instance；Skeleton会实例化其中所有的服务实例；称为multi-binding</li>
  <li><code class="language-plaintext highlighter-rouge">ara::core::InstanceSpecifier</code>: 首先通过它来解析manifest，获取<code class="language-plaintext highlighter-rouge">ara::com::InstanceIdentifier</code>，然后再通过<code class="language-plaintext highlighter-rouge">ara::com::InstanceIdentifier</code>实例化Skeleton；根据manifest，也可能是multi-binding</li>
</ul>

<h3 id="544-offering-service-instance">5.4.4 Offering Service instance</h3>

<blockquote>
  <p>From this point in time, where you call it, method calls might be dispatched to your
service instance — even if the call to OfferService() has not yet returned.</p>
</blockquote>

<p>在<code class="language-plaintext highlighter-rouge">OfferService()</code>内部，对外提供服务需要的系统资源，例如uds socket、共享内存、TCP/UDP监听端口，开始分配，并对外发送当前服务实例的存在；在<code class="language-plaintext highlighter-rouge">OfferService()</code>还未返回的时候，可能已经有Proxy的实例与当前Skeleton实例创建链接并发送订阅、method请求；Skeleton析构函数会间接停止对外提供服务，释放系统资源；也可以主动调用<code class="language-plaintext highlighter-rouge"> StopOfferService()</code>主动停止对外提供服务。</p>

<h3 id="545-polling-and-event-driven-processing-modes">5.4.5 Polling and event-driven processing modes</h3>

<p>Skeleton端的polling和event-driven主要体现在对method请求的处理上；Proxy端的polling和event-driven主要体现在对订阅的event数据处理上。在Skeleton的构造函数中，第二个参数用于指定method请求的处理方式： kPoll, kEvent, kEventSingleThread：</p>

<ul>
  <li>同一个Skeleton实例，其提供的所有方法都共用同一个处理方式</li>
  <li>当Skeleton实例是multi-binding时，Skeleton实例包含的所有服务实例的所有method都共用同一个处理方式</li>
  <li>默认是kEvent方式</li>
</ul>

<h4 id="5451-polling-mode">5.4.5.1 Polling Mode</h4>

<p>polling模式通过<code class="language-plaintext highlighter-rouge">ara::core::Future&lt;bool&gt; ProcessNextMethodCall();</code>接口实现。在polling模式下当method的request请求到达后，不会触发method的处理，而是将请求以队列的形式缓存；等待用户主动调用<code class="language-plaintext highlighter-rouge">ara::core::Future&lt;bool&gt; ProcessNextMethodCall();</code>。这个方法返回一个Future，包含了一个布尔值，表示队列中是否包含下一个待处理的method请求。</p>

<ul>
  <li>在Skeleton API中用户的method实现虚接口的返回值类型是<code class="language-plaintext highlighter-rouge">ara::core::Future</code>：这表示用户的方法可能是异步的</li>
  <li>在ProcessNextMethodCall方法中，根据底层通信协议的不同，一般应包含如下步骤：
    <ul>
      <li>将method请求数据反序列化</li>
      <li>将反序列化后的参数列表传入用户的method实现，得到一个Future：在用户的方法实现可能是同步的；也可能是异步的</li>
      <li>通过用户method实现返回的Future拿到处理结果</li>
      <li>将这个结果序列化成response，发送给Proxy</li>
      <li>检查是否存在下一个method请求，如果是返回true，否则返回false</li>
    </ul>
  </li>
</ul>

<p>在以上的步骤中，有两次返回Future：用户method处理实现返回和<code class="language-plaintext highlighter-rouge">ProcessNextMethodCall</code>返回，这两处只要有一处是异步的，则整个method请求的处理就是异步的。序列化、反序列化、检查是否有下一个请求等步骤可以在<code class="language-plaintext highlighter-rouge">ProcessNextMethodCall</code>的当前线程，也可以异步执行；用户method处理方法中，可以先返回Future，然后异步执行处理，也可以在<code class="language-plaintext highlighter-rouge">ProcessNextMethodCall</code>同一个线程中同步完成。<code class="language-plaintext highlighter-rouge">ProcessNextMethodCall</code>所在的线程、序列化/反序列化/调用用户实现/返送返回值的线程、用户实际处理method的线程，这三个上下文可以是同一个线程，可以是两个线程，也可以是三个不同的线程。</p>

<h4 id="5452-event-driven-mode">5.4.5.2 Event-Driven Mode</h4>

<p>kEvent模式和kEventSingleThread模式下，当通信协议接收到method的request请求后，会异步触发相关method处理，例如将任务放入线程池执行。kEvent模式下，请求到达后直接放入线程池处理；kEventSingleThread模式下只有一个线程，method的request按照先后顺序在同一个线程中执行。</p>

<h3 id="546-methods">5.4.6 Methods</h3>

<p>在Skeleton端用户的method实现返回一个Future这意味着，用户可以选择同步或者异步处理请求，例如用户可以在其中开启异步线程或者放入线程池，然后马上返回一个Future</p>

<h3 id="547-events">5.4.7 Events</h3>

<p>第一个发送接口使用拷贝的方式将用户数据拷贝到底层通信协议：<code class="language-plaintext highlighter-rouge">ara::core::Result&lt;void&gt; Send(const SampleType &amp;data);</code>; 为了减少拷贝AUTOSAR提供了另外一个发送接口：<code class="language-plaintext highlighter-rouge">ara::core::Result&lt;void&gt; Send(ara::com::SampleAllocateePtr&lt;SampleType&gt;data);</code>，其中<code class="language-plaintext highlighter-rouge">ara::com::SampleAllocateePtr&lt;SampleType&gt;</code>的行为可以当成<code class="language-plaintext highlighter-rouge">std::unique_ptr</code>，在调用这个发送接口前可以通过使用<code class="language-plaintext highlighter-rouge">ara::core::Result&lt;ara::com::SampleAllocateePtr&lt;SampleType&gt;&gt; Allocate();</code>接口向ComAPI层申请空间，申请到后用户将数据填充，然后调用<code class="language-plaintext highlighter-rouge">Send</code>，加入ComAPI在共享内存中开辟这些空间，则在同一个机器上的不同进程可以在Sender和Reciever之间实现<code class="language-plaintext highlighter-rouge">zero copy</code>。</p>]]></content><author><name></name></author><category term="autosar" /><summary type="html"><![CDATA[标准连接：ara::com API]]></summary></entry><entry><title type="html">pimpl vs virtual class: binary difference</title><link href="http://localhost:4000/2024/05/25/pimle-vs-virtual-class-binary-diff.html" rel="alternate" type="text/html" title="pimpl vs virtual class: binary difference" /><published>2024-05-25T10:22:46+08:00</published><updated>2024-05-25T10:22:46+08:00</updated><id>http://localhost:4000/2024/05/25/pimle-vs-virtual-class-binary-diff</id><content type="html" xml:base="http://localhost:4000/2024/05/25/pimle-vs-virtual-class-binary-diff.html"><![CDATA[<p>Both pimpl and pure abstract class can achieve compilation firewall, but they are different in theory and used in different scenarios in the real world</p>

<h3 id="technical-difference-under-the-hood">Technical difference under the hood</h3>

<h4 id="pimpl">pimpl</h4>

<p>The core of pimpl is opaque pointer. The user only knows the public APIs plus opaque pointer. During compiling time, the user code can complile only depending on the client lib through the pointer, because the pointer size is known for the user, so the user lib does not <em>directly</em> depend on the client lib binary, namely, the <em>implementation</em> details. Whether the user is shared lib or executable, there is no need for recompile or relink needed when the client side changes, because the user side only refer to implementation through pointer and this pointer <em>binding</em> will be done during runtime by dynamic linker. The client lib developer can write a dummy implementation of the client APIs for the purpose of debuging and isolation of working with the user side and there is no need for recompile or link when the dummy implementation lib is replaced with the real client lib. The development for user side and client side can be isolated this way. The core reason why pimpl works is the dynamic linker, which can resolve symbols during runtime.</p>

<p>There is additional method to achieve more efficiency, such as <code class="language-plaintext highlighter-rouge">lazy binding</code> during the process of dynamic linking.</p>

<p>Note: no additional CPU instructions are generated for using pimpl, the overhead comes from the access of the opaque pointer.</p>

<h4 id="pure-abstract-class">pure abstract class</h4>

<p>Pure abstract class use virtual table as the medium to achieve compile isolation. It’s a process called <code class="language-plaintext highlighter-rouge">late binding</code></p>

<p>Late binding, also known as dynamic binding or runtime binding, refers to the process of determining the specific function implementation to be called at runtime, based on the actual type of the object being referred to. It is typically associated with polymorphism and virtual function dispatch.</p>

<p>In languages like C++, when a virtual function is called on a pointer or reference to a base class, late binding ensures that the appropriate function implementation is selected based on the runtime type of the object. This allows different derived classes to have their own implementations of the same virtual function, providing flexibility and extensibility.</p>

<p>Note: The user use the base pure virtual class and the APIs and when the user’s code are actually executed, the API implementation is decided by the actual derived subclasses that is passed into the user’s function. At compile time the user’s code only knows the base class and will generate the CPU instructions for how to get relevent function address based on the virtual table, but at runtime, the passed instance is subclass of base class, which has a different virtual table address(but the instrunctions for finding functions are the same); This way the functions found is the implementation of the subclass. This behavior decides that if the base class’s member or methods are changed (whether it’s quantity or appearance squences, because these all influence the CPU instructions generated from the compiler), the user code need to be compiled again, while pimpl does not have this problem.</p>

<p>Note: The overhead of pure abstract class is in the access of the virtual tables. CPU instructions will be generated by the compiler to navigate to the desired implementation through pointer, with the help of the virtual table.</p>

<p>Note that pure abstract class does not prevent successuful building even if there are no implementations:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span>
<span class="k">class</span> <span class="nc">VirtualClass</span> <span class="p">{</span>
<span class="nl">public:</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">f</span><span class="p">()</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">};</span>

<span class="kt">void</span> <span class="nf">somefunc</span><span class="p">(</span><span class="n">VirtualClass</span> <span class="o">*</span><span class="n">cls</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Hello"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="c1">/// This call be compiled successfully, but will have segmentation fault at</span>
  <span class="c1">/// runtime</span>
  <span class="n">cls</span><span class="o">-&gt;</span><span class="n">f</span><span class="p">();</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">void</span> <span class="o">*</span><span class="n">ptr</span><span class="p">;</span>
  <span class="n">somefunc</span><span class="p">((</span><span class="n">VirtualClass</span> <span class="o">*</span><span class="p">)(</span><span class="n">ptr</span><span class="p">));</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Above code will compile successfully!</p>

<h3 id="use-case">Use case</h3>

<p>A pure abstract class is mainly used as interface. Namely, a group of clients has the same API and the user side only need to use the common interfaces that are shared by all the clients. It’s one to more mapping between the user and the client. The actualization is achieved by passing different client instances into the user.</p>

<p>Pimpl is mainly used for encapsulation. Namely, a class or lib is exposed to user as standard APIs(only public api that will be used by user) and the private implementations are hidden by the opaque pointer. It’s more to one mapping between user and client. The actualization is achieved by user use the standard API provided by client and link to the client lib.</p>

<h3 id="compile-fire-wall-and-linking">Compile fire wall and linking</h3>

<p>Compile isolation means there is no need for <code class="language-plaintext highlighter-rouge">static</code> recompiling and relinking during compilation time, only runtime dynamic linking(pimpl) or runtime virtual table handling(pure abstract class) is required for the client’s new implementation to take effect.</p>]]></content><author><name></name></author><category term="c++" /><summary type="html"><![CDATA[Both pimpl and pure abstract class can achieve compilation firewall, but they are different in theory and used in different scenarios in the real world]]></summary></entry></feed>